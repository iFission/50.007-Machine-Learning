Q1.1
-10 for mathematically wrong (i)
-5 for wrong result in (ii) with no working 
-4 for wrong result in (ii) with working
- No penalty for not writing φ(x)φ(y) if K is correct

Q1.2
-10 for missing Lagrange multiplier for ξ. (Otherwise it's just copying the hard margin problem)
-15 for simply copying the hard margin problem (even though it's partly correct, it's zero effort to work through the question).
-2 for each missing derivative wrt w, b, ξi
-1 for having summation after ∂L/∂ξi (mathematically incorrect, although it should not affect results). If you expand the terms to ξ1 ξ2 ξ3... you can see why all the other ξ's disappear when you differentiate L.
- No deduction for writing ∂/∂ξ instead of ξi, unless it leads to wrong result
-1 for omitting each KKT constraint: αi(di ...) = 0, βi >= 0, βiξi = 0
-1 for omitting each dual constraint ∑αidi = 0, 0<=α<=C
-1 per constraint omitted
-3 for correct L without stating dual problem
-1 for not stating max or min for dual problem

-2 for not mentioning non-linearly separable but mentioning noise/overfitting
-2 for hinting at separability ("lie on the wrong side")
-5 for saying linearly separable instead of NOT

Q1.4
-2 per different result for not using the default settings (this was explicitly stated in the question)
-2 per different result when reading the inputs wrongly
-6 total for correct code, wrong input files
-12 for wrong accuracy without code

Q2.2-3
-5 for reversed signs (no deduction for > or >=)
-3 for maths error in signs (e.g 6-x>0 then x>6)
-10 for wrong boundary
-4 for boundary without indicating which side is 1 or 0
-1 for not clearly indicating which side is 1 or 0

Q2.4
Accepted variant forms of underflow: "probability will tend to 0", "limit of precision", "not enough bits to store", "very small value", "get 0 after multiplying", etc. If you followed the hint and tried to multiply 1000 probabilities, you would see this issue, so it's not difficult.

-5 for possible reasons, such as it makes the score linear in theta, makes gradients easier to compute, prevents vanishing gradient
-7 for hinting at possible reasons
-3 for possible reasons + hinting about underflow
-5 for only hinting at underflow (e.g. "not numerically stable", "lose precision", "product gets smaller". These are valid, but happen all the time with no issue -- the real problem is reaching 0.)
-10 for not mentioning underflow issue at all, or only saying addition is faster than multiplication / sum-log is faster than log-product -- both not true, see test code below.

Float multiplications are faster than additions because the computer doesn't have to convert the exponents. Test:

import math
import random
s = random.random()
t = random.random()
s_mul = s
t_mul = t
s_add = math.log(s)
t_add = math.log(t)

%timeit s_mul * t_mul
30.2 ns ± 0.13 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
%timeit s_add * t_add
31.6 ns ± 0.592 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)


Log of products is faster than sum of logs due to 1 log instead of n log operations. Test:

import math

def sumlog(a):
    s = 0.0
    for n in a:
        s += math.log(n)
    return s

def logprod(a):
    p = 1.0
    for n in a:
        p *= n
    return math.log(p)

a = [float(i) for i in range(1,11)]

%timeit logprod(a)
415 ns ± 1.76 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)

%timeit sumlog(a)
1.41 µs ± 57.5 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)


